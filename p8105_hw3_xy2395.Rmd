---
title: "p8105_hw3_xy2395"
author: "Jack Yan"
date: "10/04/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(hexbin)
library(tidyverse)
library(readxl)
library(p8105.datasets)
library(lubridate)
library(patchwork)

theme_set(theme_bw())

```

# Problem 1

The following code trunk

* formats the data to use appropriate variable names using `clean_names()`,
* focuses on the “Overall Health” topic using `filter()`, 
* includes only responses from “Excellent” to “Poor” (i.e. no pre-collapsed categories), and
* organizes responses as a factor taking levels ordered from “Excellent” to “Poor”, using `fct_relevel`.

```{r}
tidy_brfss = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  select(-(class:question), -sample_size, -(confidence_limit_low : geo_location)) %>% 
  mutate(response = fct_relevel(response, c("Excellent", "Very good", "Good", "Fair", "Poor")))

tidy_brfss
summary(tidy_brfss)
```

##Questions

### Question 1.1

In 2002, which states were observed at 7 locations?

```{r}
states_observed_7_locs = 
  tidy_brfss %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarize(n_locations = n_distinct(locationdesc)) %>% 
  filter(n_locations == 7)

states_observed_7_locs
```

As shown above, `r states_observed_7_locs$locationabbr[1]`, `r states_observed_7_locs$locationabbr[2]` and `r states_observed_7_locs$locationabbr[3]` were observed at 7 locations in 2002.

### Question 1.2

Make a “spaghetti plot” that shows the number of **locations** in each state from 2002 to 2010.

```{r}
n_locations_in_state =
  tidy_brfss %>% 
  group_by(year, locationabbr, locationdesc) %>% 
  distinct(locationdesc) %>%  
  group_by(year, locationabbr) %>% 
  count()

n_locations_in_state %>% 
  ggplot(aes(x = year, y = n, color = locationabbr)) +
    geom_line() +
    labs(
      title = "Number of locations in each state",
      x = "Year",
      y = "Number of locations",
      color = 'States'
    ) 

n_locations_in_state %>% 
arrange(desc(n))
```

This plot shows the number of locations observed in each state varying from 2002 to 2010. For the majority of states, number of locations varied between 1 and 8 and stayed almost stable. For some other states such as FL, NJ and TX, number of locations witnessed rapid change. Most noticeably, the number of locations in FL changed rapidly in 2007, 2008 and 2010. States are discerned both by color and by numbers given above, since the number of states are too large to render a discernable color range.

### Question 1.3 

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r}
tidy_brfss %>% 
  filter(year == 2002 | year == 2006 | year == 2010) %>% 
  filter(locationabbr == "NY") %>% 
  filter(response == "Excellent") %>% 
  group_by(year) %>% 
  summarize(
    mean = mean(data_value),
    sd = sd(data_value)
  ) %>% 
  gather(key = type_of_statistics, value = value, mean:sd) %>% 
  spread(key = year, value = value) %>% 
  knitr::kable(digits = 2, 
               format = 'html', 
               caption = "Table: Mean and standard deviation of the proportion of “Excellent” responses across locations in NY State",
               col.names = c("Type of Statistics", "2002", "2006", "2010")
  )
```

This table above shows the mean and standard deviation of the proportion of "Excellent" responses across locations in NY state. Among the three years, the mean is the highest in 2002 and the lowest in 2006. The standard deviation is the highest in 2002 and the lowest in 2010. Mean and standard deviation were relatively stable in the three years.

### Question 1.4

For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r}
tidy_brfss %>% 
  group_by(year, locationabbr, response) %>% 
  summarize(mean = mean(data_value, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean, color = response)) +
    geom_point() +
    facet_grid( ~ response) +
    theme(legend.position = "bottom", 
          panel.spacing = unit(1, "lines"),
          axis.text.x = element_text(size = rel(0.8))
    ) +
    labs(
      title = "Distribution of state-level average proportion of response",
      x = "Year",
      y = "Average Proportion (%)",
      color = 'Response'
    ) 
```

As shown in the graph, the average proportion of each response among states over the years are generally similar, with a range of ~10%, and with some outliers. For each of the five responses, their proportion stayed stable across the years. This plot also helps us compare the proportion of each response.For instance, the response 'Very Good' had an overall highest proportion and the response 'Poor' had the lowest.

# Problem 2

## Description

The instacart data, held in a `tibble`, contains `r nrow(instacart)` observations and `r ncol(instacart)` variables. 

In this dataset, key variables involved in the following questions include `aisle` and `aisle_id`, which denote the names and identifiers of distinct aisles, `product_name` and `product_id`, which denote names and identifiers of distinct products (items), and `order_dow` and `order_hour_of_day`, which denote the day of the week, and the hour of the day that the order was placed, respectively. Other variables not involved in this problem may also be of interest for other purposes.

Each observation (row) in this dataset denotes a product bought in an order. Different products bought in the same order are separated in different rows. For example, as printed below, the first 5 rows in this dataset contain 15 variables. They are different products bought in order #1 by the same customer. The first product was Bulgarian Yogurt from the yogurt aisle, dairy eggs department. It was bought on 10 a.m. Wednesday. 

```{r}
options(tibble.width = Inf) 
instacart[1:5, ]
```



## Questions

### Question 2.1

How many aisles are there, and which aisles are the most items ordered from?

```{r}
instacart %>% 
  distinct(aisle) %>% 
  nrow()
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

There are `r instacart %>% distinct(aisle) %>% nrow()` aisles in this dataset. Since each observation denotes an item being bought, counting the number of observations by aisle tells us the number of items ordered from each aisle. Sorting the numbers, we find that the most items are ordered from the `fresh vegetables` aisle, followed by the `fresh fruits` aisle and `packaged vegetables fruits` aisle.

### Question 2.2

Make a plot that shows the number of items ordered in each aisle. Order aisles `sensibly`, and organize your plot so others can read it.

```{r fig.height = 8, fig.width = 13}
instacart %>% 
  count(aisle) %>% 
  mutate(aisle = fct_reorder(aisle, desc(n))) %>% 
  ggplot(aes(x = aisle, y = n)) +
  scale_y_log10(breaks = c(200000,20000, 2000, 200),labels=c("200000","20000", "2000", "200"), limits = c(200,200001)) +
    geom_point() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, size = rel(1))) +
    labs(
      title = "Number of items ordered in each aisle",
      x = "Aisle",
      y = "Number of items"
    ) 
```

This plot orderly shows number of items ordered in each aisle, using a $log_{10}$ y-scale in order to better show differences among small numbers. It shows that fresh vegetables and fresh fruits are the two most popular aisles. From this plot, we can see that the numbers of items ordered in each aisle vary a lot, ranging approximately between 200 and 200,000.

### Question 2.3
Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. 

```{r}
instacart %>% 
  filter(aisle %in% c('baking ingredients', 'dog food care', 'packaged vegetables fruits')) %>% 
  group_by(aisle, product_name)  %>% 
  count() %>% 
  group_by(aisle) %>% 
  filter(n == max(n)) %>% # This step is tricky
  select(aisle, product_name) %>% 
  knitr::kable(
    col.names = c("Aisle", "Most popular item"), 
    format = 'html', 
    caption = "Table: The most popular item in three aisles"
  )
```

The most popular item in aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits” are Light Brown Sugar, Snack Sticks Chicken & Rice Recipe Dog Treats, and Organic Baby Spinach, respectively.

### Question 2.4

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
instacart %>% 
  mutate(order_dow = factor(order_dow, labels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"))) %>% 
  filter(product_name %in% c('Pink Lady Apples', "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean) %>% 
  knitr::kable(digit = 2, 
               format = 'html', 
               col.names = c("Product Name", "Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"),
               caption = "Table: Mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered"
  )
  
```

Mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered follow different patterns. Coffee Ice Cream is on average ordered in the afternoons (around 1430) with the only exception of Friday. For 4 days in a week, Pink Lady Apples are ordered on average before noon (around 1145). In general, the time in a day Pink Lady Apples are ordered is before the time in a day Coffee Ice Cream is ordered.

```{r}
instacart %>% 
  mutate(order_dow = factor(order_dow, labels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"))) %>% 
  filter(product_name %in% c('Pink Lady Apples', "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean = mean(order_hour_of_day)) %>% 
  ggplot(aes(x = order_dow, y = mean, color = product_name)) +
    geom_point() 
  
```

In this case, a plot may be more informative than table.

# Problem 3

## Description

The data contains `r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` variables. The dataset is stored in a `tibble`. 5 variables in the original dataset stores weather data: `prcp` means precipitation in tenths of mm, `snow` denotes snowfall in mm, `snwd` means snow depth in mm, and `tmax` and `tmin` denotes maximum and minimum temperature in tenths of degrees C, respectively. Additionally, `id` denotes ID of the station conducting this observation, and `date` indicates the date when observation was made. According to Jeff, 'each weather station may collect only a subset of these variables, so the resulting dataset contains extensive missing data.' However, given the large number of weather stations, the data collectively can as well reflect the weather condition in New York. Therefore, missing data would not be a fatal problem for our analysis.

## Questions

### Question 3.1

Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

```{r}
# do some data cleaning
tidy_ny_noaa = ny_noaa %>% 
  mutate(
    year = as.integer(year(date)),
    month = as.integer(month(date)),
    day = as.integer(day(date)),
    tmax = as.numeric(tmax) / 10,
    tmin = as.numeric(tmin) / 10,
    prcp = prcp / 10
  ) %>% 
  select(id, date, year, month, day, everything())

head(tidy_ny_noaa)

tidy_ny_noaa %>% 
  count(snow) %>% 
  arrange(desc(n)) %>% 
  head()

```

For snowfall, `0`s are the most commonly observed values because for most of the days it did not snow in New York.

### Question 3.2

Make a two-panel plot showing the average **max** temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
tidy_ny_noaa %>% 
  filter(is.na(tmax) == FALSE) %>% 
  filter(month == 7 | month == 1) %>% 
  group_by(year, month,id) %>%
  summarize(avg_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = avg_tmax, group = year)) +
    geom_boxplot() +
    facet_grid(~month, labeller = as_labeller(c("1" = "Jan", "7" = "Jul"))) +
    labs(
      title = "Average max temperature in January and in July in each station across years",
      x = "Year",
      y = "Average Maximum Temperature (C)"
    )
```

The average maximum temperature is higher in July than in January for each year across all the weather stations. The temperature is around 0 C in January and around 27 C in July. Most outliers in January are higher temperatures and most outliers in July are lower temperatures. This indicates that in Janurary, some parts of the city had remarkably higher temperature than the city's general temperature, and in July, some parts of the city had remarkably lower temperature than the city's temperature in general. 


### Question 3.3

Make a two-panel plot showing 
(i) tmax vs tmin for the full dataset (note that a scatterplot may NOT be the best option), and
(ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
# plot the 1st panel
plot_temp = 
  tidy_ny_noaa %>% 
  filter(is.na(tmax) == FALSE,  is.na(tmin) == FALSE) %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex(show.legend = T) +
  labs(
      title = "Maximum vs Minimum Temperature",
      x = "Minimum Temperature (C)",
      y = "Maximum Temperature (C)"
  ) +
  theme(legend.position = "right")

```

```{r fig.height = 6, fig.width = 13}
# plot the 2nd panel
plot_snow = 
tidy_ny_noaa %>% 
  filter(snow > 0, snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot(aes(group=year)) +
  labs(
      title = "Distribution of snowfall",
      x = "Year",
      y = "Snowfall (mm)"
  ) 

# combine the two plots using patchwork
plot_temp + plot_snow
```

A scatterplot can only represent point density up to a certain threshold. Since the number of points is extremely large and we want to see the overall distribution of `tmax` against `tmin`, it is better that we use hexagonal heatmap. Looking at the plot, we can see a light linear region inside the outer darker region. Because lighter color indicates more counts, this pattern suggests that most of the max temperature and min temperature are correlated.

In this case, using boxplot can help us see the outliers more clearly. As shown in the second plot, for most years (e.g. 1980 - 1997), the distribution of snowfall remained stable. However, 2006 witnessed a noticeably lower amount of snowfall, and the outliers in this year are the most. 


